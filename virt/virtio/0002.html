
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

    <title>Deep dive into Virtio-networking and vhost-net &#8212; GDocs 0.1 documentation</title>
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/alabaster.css" />
    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/sphinx_highlight.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="How vhost-user came into being: Virtio-networking and DPDK" href="0003.html" />
    <link rel="prev" title="Introduction to virtio-networking and vhost-net" href="0001.html" />
   
  <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />
  

  
  

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="deep-dive-into-virtio-networking-and-vhost-net">
<span id="virtio-networking-and-vhost-net"></span><h1><a class="toc-backref" href="#id1" role="doc-backlink">Deep dive into Virtio-networking and vhost-net</a><a class="headerlink" href="#deep-dive-into-virtio-networking-and-vhost-net" title="Permalink to this heading">¶</a></h1>
<nav class="contents" id="table-of-contents">
<p class="topic-title">Table of Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#deep-dive-into-virtio-networking-and-vhost-net" id="id1">Deep dive into Virtio-networking and vhost-net</a></p>
<ul>
<li><p><a class="reference internal" href="#previous-concepts" id="id2">Previous Concepts</a></p></li>
<li><p><a class="reference internal" href="#networking" id="id3">Networking</a></p></li>
<li><p><a class="reference internal" href="#ipc-system-programming" id="id4">IPC, System programming</a></p></li>
<li><p><a class="reference internal" href="#qemu-and-device-emulation" id="id5">QEMU and device emulation</a></p></li>
<li><p><a class="reference internal" href="#kvm" id="id6">KVM</a></p></li>
<li><p><a class="reference internal" href="#the-virtio-specification" id="id7">The virtio specification</a></p>
<ul>
<li><p><a class="reference internal" href="#virtio-specification-devices-and-drivers" id="id8">Virtio specification: devices and drivers</a></p></li>
<li><p><a class="reference internal" href="#virtio-specification-virtqueues" id="id9">Virtio specification: virtqueues</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#networking-with-virtio-qemu-implementation" id="id10">Networking with virtio: qemu implementation</a></p></li>
<li><p><a class="reference internal" href="#vhost-protocol" id="id11">Vhost protocol</a></p>
<ul>
<li><p><a class="reference internal" href="#introduction" id="id12">Introduction</a></p></li>
<li><p><a class="reference internal" href="#vhost-net" id="id13">Vhost-net</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#communication-with-the-outside-world" id="id14">Communication with the outside world</a></p></li>
</ul>
</li>
</ul>
</nav>
<p>Copy from <a class="reference external" href="https://www.redhat.com/en/blog/deep-dive-virtio-networking-and-vhost-net">https://www.redhat.com/en/blog/deep-dive-virtio-networking-and-vhost-net</a></p>
<p>In this post we will explain the vhost-net architecture described in the <a class="reference internal" href="0001.html#introduction-virtio-networking-vhost-net"><span class="std std-ref">introduction</span></a>, to make it clear how everything works together from a technical point of view. This is part of the series of blogs that introduces you to the realm of virtio-networking which brings together the world of virtualization and the world of networking.</p>
<p>This post is intended for architects and developers who are interested in understanding what happens under the hood of the vhost-net/virtio-net architecture described in the previous blog.</p>
<p>We’ll start by describing how the different virtio spec standard components and shared memory regions are arranged in the hypervisor, how QEMU emulates a virtio network device and how the guest uses the open virtio specification to implement the virtualized driver for managing and communicating with that device.</p>
<p>After showing you the QEMU virtio architecture we will analyze the I/O bottlenecks and limitations and we will use the host’s kernel to overcome them, reaching the vhost-net architecture presented in the overview post (link).</p>
<p>Last, but not least, we will show how to connect the virtual machine to the external word, beyond the host it’s running on, using Open Virtual Switch (OVS), an open source virtual, SDN-capable, distributed switch.</p>
<p>By the end of this post, you’ll be able to understand how the vhost-net/virtio-net architecture works, the purpose of each of its components and how packets are sent and received.</p>
<section id="previous-concepts">
<h2><a class="toc-backref" href="#id2" role="doc-backlink">Previous Concepts</a><a class="headerlink" href="#previous-concepts" title="Permalink to this heading">¶</a></h2>
<p>In this section we will briefly explain a few concepts that you need to know in order to fully understand this post. It might seem basic for the well-versed in the matter, but it will provide a common foundation to build on.</p>
</section>
<section id="networking">
<h2><a class="toc-backref" href="#id3" role="doc-backlink">Networking</a><a class="headerlink" href="#networking" title="Permalink to this heading">¶</a></h2>
<p>Let’s start with the basics. A physical NIC (Network Interface Card) is the hardware (real) component that allows the host to connect to the external world. It can perform some offloading, like performing a checksum calculation in the NIC instead of the CPU, Segmentation Offload (fragment a larger piece of data into small chunks, like ethernet MTU size) or Large Receive Offload (join many received packets’ data into only one for the CPU point of view).</p>
<p>On the other side we have tun/tap devices, virtual point-to-point network devices that the userspace applications can use to exchange packets. The device is called a tap device when the data exchanged is layer 2 (ethernet frames), and a tun device if the data exchanged is layer 3 (IP packets).</p>
<p>When the tun kernel module is loaded it creates a special device /dev/net/tun. A process can create a tap device opening it and sending special ioctl commands to it. The new tap device has a name in the /dev filesystem and another process can open it, send and receive Ethernet frames.</p>
</section>
<section id="ipc-system-programming">
<h2><a class="toc-backref" href="#id4" role="doc-backlink">IPC, System programming</a><a class="headerlink" href="#ipc-system-programming" title="Permalink to this heading">¶</a></h2>
<p>Unix sockets are a way to do Inter-Process Communication (IPC) on the same machine in an efficient way. In this post scope, the server of the communication binds a Unix socket to a path in the file system, so a client can connect to it using that path. From that moment, the processes can exchange messages. Note that unix sockets can also be used to exchange file descriptors between processes.</p>
<p>An eventfd is a lighter way of performing IPC. While Unix sockets allows to send and receive any kind of data, eventfd is only an integer that a producer can change and a consumer can poll and read. This makes them more suitable as a wait/notify mechanism, rather than information passing.</p>
<p>Both of these IPC systems expose a file descriptor for each process in the communication. The fcntl call performs different operations on that file descriptors, as making them non-blocking (so a read operation returns immediately if there is nothing to read). The ioctl call follows the same pattern, but implementing device-specific operations, like sending commands.</p>
<p>Shared memory is the last method of IPC we will cover here. Instead of providing a channel to communicate two process, it makes some of the processes’ memory regions point to the same memory page, so the change that one process writes over it affects the subsequent reads the other make.</p>
</section>
<section id="qemu-and-device-emulation">
<h2><a class="toc-backref" href="#id5" role="doc-backlink">QEMU and device emulation</a><a class="headerlink" href="#qemu-and-device-emulation" title="Permalink to this heading">¶</a></h2>
<p>QEMU is a hosted virtual machine emulator that provides a set of different hardware and device models for the guest machine. For the host, qemu appears as a regular process scheduled by the standard Linux scheduler, with its own process memory. In the process, QEMU allocates a memory region that the guest see as physical, and executes the virtual machine’s CPU instructions.</p>
<p>To perform I/O on bare metal hardware, like storage or networking, the CPU has to interact with physical devices performing special instructions and accessing particular memory regions, such as the ones that the device is mapped to.</p>
<p>When the guests access these memory regions, control is returned to QEMU, which performs the device’s emulation in a transparent manner for the guest.</p>
</section>
<section id="kvm">
<h2><a class="toc-backref" href="#id6" role="doc-backlink">KVM</a><a class="headerlink" href="#kvm" title="Permalink to this heading">¶</a></h2>
<p>Kernel-based Virtual Machine (KVM) is an open source virtualization technology built into Linux. It provides hardware assist to the virtualization software, using built-in CPU virtualization technology to reduce virtualization overheads (cache, I/O, memory) and improving security.</p>
<p>With KVM, QEMU can just create a virtual machine with virtual CPUs (vCPUs) that the processor is aware of, that runs native-speed instructions. When a special instruction is reached by KVM, like the ones that interacts with the devices or to special memory regions, vCPU pauses and informs QEMU of the cause of pause, allowing hypervisor to react to that event.</p>
<p>In the regular KVM operation, the hypervisor opens the device /dev/kvm, and communicates with it using ioctl calls to create the VM, add CPUs, add memory (allocated by qemu, but physical from the virtual machine’s point of view), send CPU interrupts (as an external device would send), etc. For example, one of these ioctl runs the actual KVM vCPU,, blocking QEMU and making the vCPU run until it found an instruction that needs hardware assistance. In that moment, the ioctl returns (this is called vmexit) and QEMU knows the cause of that exit (for example, the offending instruction).</p>
<p>For special memory regions, KVM follows a similar approach, marking memory regions as Read Only or not mapping them at all, causing a vmexit with the KVM_EXIT_MMIO reason.</p>
</section>
<section id="the-virtio-specification">
<h2><a class="toc-backref" href="#id7" role="doc-backlink">The virtio specification</a><a class="headerlink" href="#the-virtio-specification" title="Permalink to this heading">¶</a></h2>
<section id="virtio-specification-devices-and-drivers">
<h3><a class="toc-backref" href="#id8" role="doc-backlink">Virtio specification: devices and drivers</a><a class="headerlink" href="#virtio-specification-devices-and-drivers" title="Permalink to this heading">¶</a></h3>
<p>Virtio is an open specification for virtual machines’ data I/O communication, offering a straightforward, efficient, standard and extensible mechanism for virtual devices, rather than boutique per-environment or per-OS mechanisms. It uses the fact that the guest can share memory with the host for I/O to implement that.</p>
<p>The virtio specification is based on two elements: devices and drivers. In a typical implementation, the hypervisor exposes the virtio devices to the guest through a number of transport methods. By design they look like physical devices to the guest within the virtual machine.</p>
<p>The most common transport method is PCI or PCIe bus. However, the device can be available at some predefined guest’s memory address (MMIO transport). These devices can be completely virtual with no physical counterpart or physical ones exposing a compatible interface.</p>
<p>The typical (and easiest) way to expose a virtio device is through a PCI port since we can leverage the fact that PCI is a mature and well supported protocol in QEMU and Linux drivers. Real PCI hardware exposes its configuration space using a specific physical memory address range (i.e., the driver can read or write the device’s registers by accessing that memory range) and/or special processor instructions. In the VM world, the hypervisor captures accesses to that memory range and performs device emulation, exposing the same memory layout that a real machine would have and offering the same responses. The virtio specification also defines the layout of its PCI Configuration space, so implementing it is straightforward.</p>
<p>When the guest boots and uses the PCI/PCIe auto discovering mechanism, the virtio devices identify themselves with with the PCI vendor ID and their PCI Device ID. The guest’s kernel uses these identifiers to know which driver must handle the device. In particular, the linux kernel already includes virtio drivers.</p>
<p>The virtio drivers must be able to allocate memory regions that both the hypervisor and the devices can access for reading and writing, i.e., via memory sharing. We call data plane the part of the data communication that uses these memory regions, and control plane the process of setting them up. We will provide further details on the virtio protocol implementation and memory layout in future posts.</p>
<p>The virtio kernel drivers share a generic transport-specific interface (e.g: virtio-pci), used by the actual transport and device implementation (such as virtio-net, or virtio-scsi).</p>
</section>
<section id="virtio-specification-virtqueues">
<h3><a class="toc-backref" href="#id9" role="doc-backlink">Virtio specification: virtqueues</a><a class="headerlink" href="#virtio-specification-virtqueues" title="Permalink to this heading">¶</a></h3>
<p>Virtqueues are the mechanism for bulk data transport on virtio devices. Each device can have zero or more virtqueues (<a class="reference external" href="https://docs.oasis-open.org/virtio/virtio/v1.1/cs01/virtio-v1.1-cs01.html#x1-230005">link</a>). It consists of a queue of guest-allocated buffers that the host interacts with either by reading them or by writing to them. In addition, the virtio specification also defines bi-directional notifications:</p>
<ul class="simple">
<li><p>Available Buffer Notification: Used by the driver to signal there are buffers that are ready to be processed by the device</p></li>
<li><p>Used Buffer Notification: Used by the device to signal that it has finished processing some buffers.</p></li>
</ul>
<p>In the PCI case, the guest sends the available buffer notification by writing to a specific memory address, and the device (in this case, QEMU) uses a vCPU interrupt to send the used buffer notification.</p>
<p>The virtio specification also allows the notifications to be enabled or disabled dynamically. That way, devices and drivers can batch buffer notifications or even actively poll for new buffers in virtqueues (busy polling). This approach is better suited for high traffic rates.</p>
<p>In summary, the virtio driver interface exposes:</p>
<ul class="simple">
<li><p>Device’s feature bits (which device and guest have to negotiate)</p></li>
<li><p>Status bits</p></li>
<li><p>Configuration space (that contains device specific information, like MAC address)</p></li>
<li><p>Notification system (configuration changed, buffer available, buffer used)</p></li>
<li><p>Zero or more virtqueues</p></li>
<li><p>Transport specific interface to the device</p></li>
</ul>
</section>
</section>
<section id="networking-with-virtio-qemu-implementation">
<h2><a class="toc-backref" href="#id10" role="doc-backlink">Networking with virtio: qemu implementation</a><a class="headerlink" href="#networking-with-virtio-qemu-implementation" title="Permalink to this heading">¶</a></h2>
<img alt="../../_images/virtio-networking-fig1.png" src="../../_images/virtio-networking-fig1.png" />
<p>The virtio network device is a virtual ethernet card, and it supports multiqueue for TX/RX. Empty buffers are placed in N virtqueues for receiving packets, and outgoing packets are enqueued into another N virtqueues for transmission. Another virtqueue is used for driver-device communication outside of the data plane, like to control advanced filtering features, settings like the mac address, or the number of active queues. As a physical NIC, the virtio device supports features such as many offloadings, and can let the real host’s device do them.</p>
<p>To send a packet, the driver sends to the device a buffer that includes metadata information such as desired offloadings for the packet, followed by the packet frame to transmit. The driver can also split the buffer into multiple gather entries, e.g. it can split the metadata header from the packet frame.</p>
<p>These buffers are managed by the driver and mapped by the device. In this case the device is “inside” the hypervisor. Since the hypervisor (qemu) has access to all the guests’ memory it is capable of locating the buffers and reading or writing them.</p>
<p>The following flow diagram shows the virtio-net device configuration and the sending of a packet using virtio-net driver, that communicates with the virtio-net device over PCI. After filling the packet to be sent, it triggers an “available buffer notification”, returning the control to QEMU so it can send the packet through the TAP device.</p>
<p>Qemu then notifies the guest that the buffer operation (reading or writing) is done, and it does that by placing the data in the virtqueue and sending a used notification event, triggering an interruption in the guest vCPU.</p>
<p>The process of receiving a packet is similar to that of sending it. The only difference is that, in this case, empty buffers are pre-allocated by the guest and made available to the device so it can write the incoming data to them.</p>
<img alt="../../_images/virtio-networking-fig2.png.jpg" src="../../_images/virtio-networking-fig2.png.jpg" />
</section>
<section id="vhost-protocol">
<h2><a class="toc-backref" href="#id11" role="doc-backlink">Vhost protocol</a><a class="headerlink" href="#vhost-protocol" title="Permalink to this heading">¶</a></h2>
<section id="introduction">
<h3><a class="toc-backref" href="#id12" role="doc-backlink">Introduction</a><a class="headerlink" href="#introduction" title="Permalink to this heading">¶</a></h3>
<p>The previous approach contains a few inefficiencies:</p>
<ul class="simple">
<li><p>After the virtio driver sends an Available Buffer Notification, the vCPU stops running and control is returned to the hypervisor causing an expensive context switch.</p></li>
<li><p>QEMU additional tasks/threads synchronization mechanisms.</p></li>
<li><p>The syscall and the data copy for each packet to actually send or receive it via tap (no batching).</p></li>
<li><p>The ioctl to send the available buffer notification (vCPU interruption).</p></li>
<li><p>We also need to add another syscall to resume vCPU execution, with all the associated mapping switching, etc.</p></li>
</ul>
<p>In order to address these limitations, the vhost protocol was designed. The vhost API is a message based protocol that allows the hypervisor to offload the data plane to another component (handler) that performs data forwarding more efficiently. Using this protocol, the master sends the following configuration information to the handler:</p>
<ul class="simple">
<li><p>The hypervisor’s memory layout. This way, the handler can locate the virtqueues and buffer within the hypervisor’s memory space.</p></li>
<li><p>A pair of file descriptors that are used for the handler to send and receive the notifications defined in the virtio spec. These file descriptors are shared between the handler and KVM so they can communicate directly without requiring the hypervisor’s intervention. Note that this notifications can still be dynamically disabled per virtqueue.</p></li>
</ul>
<p>After this process, the hypervisor will no longer process packets (read or write to/from the virtqueues). Instead, the dataplane will be completely offloaded to the handler, which can now access the virtqueues’ memory region directly as well as send and receive notifications directly to and from the guest.</p>
<p>The vhost messages can be exchanged in any host-local transport protocol, such as Unix sockets or character devices and the hypervisor can act as a server or as a client (in the context of the communication channel). The hypervisor is the leader of the protocol, the offloading device is a handler and any of them can send messages.</p>
<p>In order to further understand the benefits of this protocol, we will analyze the details of a kernel-based implementation of the vhost protocol: the vhost-net kernel driver.</p>
</section>
<section id="vhost-net">
<h3><a class="toc-backref" href="#id13" role="doc-backlink">Vhost-net</a><a class="headerlink" href="#vhost-net" title="Permalink to this heading">¶</a></h3>
<p>The vhost-net is a kernel driver that implements the handler side of the vhost protocol to implement an efficient data plane, i.e., packet forwarding. In this implementation, qemu and the vhost-net kernel driver (handler) use ioctls to exchange vhost messages and a couple of eventfd-like file descriptors called irqfd and ioeventfd are used to exchange notifications with the guest.</p>
<p>When vhost-net kernel driver is loaded, it exposes a character device on /dev/vhost-net. When qemu is launched with vhost-net support it opens it and initializes the vhost-net instance with several ioctl(2) calls. These are necessary to associate the hypervisor process with the vhost-net instance, prepare for virtio feature negotiation and pass the guest physical memory mapping to the vhost-net driver.</p>
<p>During the initialization the vhost-net kernel driver creates a kernel thread called vhost-$pid, where $pid is the hypervisor process pid. This thread is called the “vhost worker thread”.</p>
<p>A tap device is still used to communicate the VM with the host but now the worker thread handles the I/O events i.e. it polls for driver notifications or tap events, and forwards data.</p>
<p>Qemu allocates one eventfd and registers it to both vhost and KVM in order to achieve the notification bypass. The vhost-$pid kernel thread polls it, and KVM writes to it when the guest writes in a specific address. This mechanism is named ioeventfd. This way, a simple read/write operation to a specific guest memory address does not need to go through the expensive QEMU process wakeup and can be routed to the vhost worker thread directly. This also has the advantage of being asynchronous, no need for the vCPU to stop (so no need to do an immediate context switch).</p>
<p>On the other hand, qemu allocates another eventfd and registers it to both KVM and vhost again for direct vCPU interruption injection. This mechanism is called irqfd, and it allows any process in the host to inject vCPU interrupts to the guest by writing to it, with the same advantages (asynchronous, no need for immediate context switching, etc).</p>
<p>Note that such changes in the virtio packet processing backend are completely transparent to the guest who still uses the standard virtio interface.</p>
<p>The following block and flow diagrams show the datapath offloading from qemu to the vhost-net kernel driver:</p>
<img alt="../../_images/virtio-networking-fig3.png" src="../../_images/virtio-networking-fig3.png" />
<img alt="../../_images/virtio-networking-fig4.png" src="../../_images/virtio-networking-fig4.png" />
</section>
</section>
<section id="communication-with-the-outside-world">
<h2><a class="toc-backref" href="#id14" role="doc-backlink">Communication with the outside world</a><a class="headerlink" href="#communication-with-the-outside-world" title="Permalink to this heading">¶</a></h2>
<p>The guest can communicate with the host using the tap device, however the issue remains of how does it communicate with other VMs on the same host or with machines outside the host (e.g.: with the internet)</p>
<p>We could achieve this by using any forwarding or routing mechanism provided by the kernel networking stack, like standard Linux bridges. However, a more advanced solution is to use a fully virtualized, distributed, managed switch, such as Open Virtual Switch (OVS).</p>
<p>As said in the overview post, OVS datapath is running as a kernel module in this scenario, ovs-vswitchd as a userland control and managing daemon and ovsdb-server as the forwarding database.</p>
<p>As illustrated in the following figure, OVS datapath is running in the kernel and is forwarding the packets between the physical NIC and the virtual TAP device:</p>
<img alt="../../_images/virtio-networking-fig5.png" src="../../_images/virtio-networking-fig5.png" />
<p>We can extend this case to multiple VMs running on the same host environment each with their on qemu process, TAP port and vhost-net driver which helps avoid the qemu context switches.</p>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../index.html">GDocs</a></h1>








<h3>Navigation</h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../links.html">Links</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../corp.html">Corporation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cmds/index.html">Commands</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../kernel/index.html">Linux Kernel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../python/index.html">Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../binaryAnalysis.html">BinaryAnalysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cplus/index.html">Cplus</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../els/index.html">ESL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../andorid/index.html">Andorid</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../yocto/index.html">Yocto</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../emacs/index.html">Emacs</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Virtulization</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../qemu/index.html">Qemu</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="index.html">Virtio</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="0001-pre.html">Virtio devices and drivers overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="0001.html">Introduction to virtio-networking and vhost-net</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Deep dive into Virtio-networking and vhost-net</a></li>
<li class="toctree-l3"><a class="reference internal" href="0003.html">How vhost-user came into being: Virtio-networking and DPDK</a></li>
<li class="toctree-l3"><a class="reference internal" href="0004.html">A journey to the vhost-users realm</a></li>
<li class="toctree-l3"><a class="reference internal" href="0005.html">Introduction to VirtIO</a></li>
<li class="toctree-l3"><a class="reference internal" href="0006.html">Qemu: Virtio-net Envrionment</a></li>
<li class="toctree-l3"><a class="reference internal" href="0007.html">Qemu: Virtio_net_pci</a></li>
<li class="toctree-l3"><a class="reference internal" href="0009.html">Qemu: Virtio_bus</a></li>
<li class="toctree-l3"><a class="reference internal" href="0008.html">Qemu: Virtio_net Device</a></li>
<li class="toctree-l3"><a class="reference internal" href="vhost-user.html">Vhost-user Protocol</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../00001.html">Install Ret Hat 7 server</a></li>
<li class="toctree-l2"><a class="reference internal" href="../00002.html">Qemu ifup script</a></li>
<li class="toctree-l2"><a class="reference internal" href="../00003.html">Random Mac address</a></li>
<li class="toctree-l2"><a class="reference internal" href="../00004.html">Assign tap to VM Client</a></li>
<li class="toctree-l2"><a class="reference internal" href="../00005.html">Open vSwitch with Libvirt</a></li>
<li class="toctree-l2"><a class="reference internal" href="../00006.html">EAL: No free hugepages reported in hugepages-2048kB</a></li>
<li class="toctree-l2"><a class="reference internal" href="../00007.html">Failed to connect socket /var/run/openvswitch/centos1_eth0: Permission denied</a></li>
<li class="toctree-l2"><a class="reference internal" href="../00008.html">Establish OVP reference envrionment</a></li>
<li class="toctree-l2"><a class="reference internal" href="../00009.html">libvirt: Specify the kernel and rootfs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../00010.html">WRLinux with wayland and weston</a></li>
<li class="toctree-l2"><a class="reference internal" href="../00011.html">Enable the support console for VM</a></li>
<li class="toctree-l2"><a class="reference internal" href="../00012.html">Start Qemu with monitor</a></li>
<li class="toctree-l2"><a class="reference internal" href="../00013.html">Creating images from ISO</a></li>
<li class="toctree-l2"><a class="reference internal" href="../00014.html">Create image from Wrlinux image</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../debian/index.html">Debian</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tools/index.html">Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../licence/index.html">OpenSource License</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../bochs/index.html">Bochs IA-32 Emulator</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../system/index.html">System</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../unix/index.html">UNIX</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../math/index.html">Math</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../todo.html">To Do</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../index.html">Documentation overview</a><ul>
  <li><a href="../index.html">Virtulization</a><ul>
  <li><a href="index.html">Virtio</a><ul>
      <li>Previous: <a href="0001.html" title="previous chapter">Introduction to virtio-networking and vhost-net</a></li>
      <li>Next: <a href="0003.html" title="next chapter">How vhost-user came into being: Virtio-networking and DPDK</a></li>
  </ul></li>
  </ul></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &#169;2022, Haitao Lau.
      
      |
      Powered by <a href="https://www.sphinx-doc.org/">Sphinx 5.3.0</a>
      &amp; <a href="https://alabaster.readthedocs.io">Alabaster 0.7.16</a>
      
      |
      <a href="../../_sources/virt/virtio/0002.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>